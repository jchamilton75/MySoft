# -*- coding: utf-8 -*-
"""EssaiFullSkyAlm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HyvrlFTBjMiNWV_WSdSZ1TS4HGYh099Y
"""

#!pip install camb
#!pip install healpy

import camb
from camb import model, initialpower
import healpy as hp 
import numpy as np
from pylab import *
rcParams['image.cmap'] = 'jet'

#Set up a new set of parameters for CAMB
pars = camb.CAMBparams()
#This function sets up CosmoMC-like settings, with one massive neutrino and helium set using BBN consistency
pars.set_cosmology(H0=67.5, ombh2=0.022, omch2=0.122, mnu=0.06, omk=0, tau=0.06)
pars.InitPower.set_params(ns=0.965, r=0)
pars.set_for_lmax(2500, lens_potential_accuracy=0);
#calculate results for these parameters
results = camb.get_results(pars)
#get dictionary of CAMB power spectra
powers =results.get_cmb_power_spectra(pars, CMB_unit='muK')
for name in powers: print(name)


#plot the total lensed CMB power spectra versus unlensed, and fractional difference
totCL=powers['total']
unlensedCL=powers['unlensed_scalar']
print(totCL.shape)

ls = np.arange(totCL.shape[0])
clf()
plot(ls,totCL[:,0], color='k')

CL = totCL[:,0]/ls/(ls+1)
CL[0]=0

ns = 16
map =hp.synfast(CL[0:5*ns], ns, pixwin=False)
hp.mollview(map)

lmax = 2*ns-1
nl = 2*ns
nalm = (2*ns)*(2*ns+1)/2
outcl, outalm = hp.anafast(map,alm=True, lmax=lmax)
print(outcl.shape, nl)
print(outalm.shape, nalm)
print(outalm.dtype)
print(outalm[0:50])
ll = np.arange(outcl.shape[0])
clf()
plot(ll,ll*(ll+1)*outcl)
plot(ls, ls*(ls+1)*CL, 'r')
xlim(0,max(ll))

### Target power spectra
clt = CL[0:nl]
lt = ll[0:nl]

nbmodels = 100000
nnn = int(nbmodels/30)
npixok = 12*ns**2
limit_shape = 3*ns
okpix = np.arange(npixok)
mymaps = np.zeros((nbmodels, npixok))
myalms = np.zeros((nbmodels, nalm), dtype=complex128)
expcls = np.zeros((nbmodels, nl))
mycls = np.zeros((nbmodels, nl))
expcls = np.zeros((nbmodels, nl))
allshapes = np.zeros((nbmodels, len(ls)))
for i in xrange(nbmodels):
  ylo = np.random.rand()*2
  yhi = np.random.rand()*2
  if (i/nnn)*nnn == i: 
    print(i,ylo,yhi)
  theshape = ylo+(yhi-ylo)/(limit_shape)*ls
  theshape[theshape < 0] = 0
  theshape[limit_shape:] = 0
  allshapes[i,:] = theshape
  theCL = CL*theshape
  themap = hp.synfast(theCL, ns, pixwin=False, verbose=False)
  mymaps[i,:] = themap[okpix]
  expcls[i,:], myalms[i,:] = hp.anafast(themap, lmax=lmax, alm=True)
  mycls[i,:] = theCL[0:nl]

for i in xrange(nbmodels):
  if (i/nnn)*nnn == i:
    plot(lt, lt*(lt+1)*mycls[i,:])
    
figure()
xlim(0,3*ns+10)
for i in xrange(nbmodels):
  if (i/nnn)*nnn == i:
    plot(ls, allshapes[i,:])

num = np.random.randint(0,nbmodels)
#figure()
#hp.mollview(mymaps[num,:])

figure()
plot(lt, lt*(lt+1)*mycls[num,:])
plot(lt, lt*(lt+1)*expcls[num,:])

### Deep Networks Configuration for the case of alm -> spectra
import keras
from keras.models import Sequential
model = Sequential()

from keras.layers import Dense
model.add(Dense(units=nalm*6, activation='relu', input_dim=nalm, kernel_initializer='uniform'))
#model.add(Dense(units=nalm/2, activation='linear'))
model.add(Dense(units=nl, activation='linear'))

model.compile(optimizer='adam',
              loss='mse',
              metrics=['accuracy'])

# Training
fraction = 0.8
ilim = int(nbmodels*fraction)
print(ilim)

mx = np.max(np.abs(myalms))
my = np.max(mycls)

from __future__ import print_function
class PrintNum(keras.callbacks.Callback):
  def on_epoch_end(self,epoch,logs):
    if epoch % 10 == 0: 
      print('')
      print(epoch, end='')
    sys.stdout.write('.')
    sys.stdout.flush()

early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)


history=model.fit(myalms[0:ilim,:]/mx, mycls[0:ilim]/my, epochs=200, batch_size=1000, validation_split=0.1, verbose=0, callbacks=[early_stop, PrintNum()])

# summarize history for loss
plot(history.history['loss'])
plot(history.history['val_loss'])
title('model loss')
ylabel('loss')
xlabel('epoch')
legend(['train', 'test'], loc='upper left')
yscale('log')
show()
print(min(history.history['loss']), min(history.history['val_loss']), len(history.history['val_loss']))

myalms_test = myalms[ilim:,:]
mycls_test = mycls[ilim:,:]
expcls_test = expcls[ilim:,:]

result = my * model.predict(myalms_test / mx, batch_size=128)

num=np.random.randint(result.shape[0])
plot(lt, lt*(lt+1)*mycls_test[num,:],label ='Input spectra')
plot(lt, lt*(lt+1)*expcls_test[num,:],label ='Anafast')
plot(lt, lt*(lt+1)*result[num,:],label ='ML')
title(num)
legend()

figure()
a=hist(np.ravel(expcls_test[:,2:]-mycls_test[:,2:]), bins=100, range=[-5,5], alpha=0.5, label='Anafast')
a=hist(np.ravel(result[:,2:]-mycls_test[:,2:]), bins=100, range=[-5,5], alpha=0.5, label = 'ML')
yscale('log')
legend()

ch2anafast = np.sum((expcls_test[:,2:]-mycls_test[:,2:])**2, axis=1)
ch2ML = np.sum((result[:,2:]-mycls_test[:,2:])**2, axis=1)

figure()
a=hist(ch2anafast, bins=100, range=[0,10000], alpha=0.5, label='Anafast')
a=hist(ch2ML, bins=100, range=[0,10000], alpha=0.5, label = 'ML')
yscale('log')
legend()








### Deep Networks Configuration for the case of T -> spectra
import keras
from keras.models import Sequential
model_T = Sequential()

from keras.layers import Dense
model_T.add(Dense(units=npixok*2, activation='relu', input_dim=npixok, kernel_initializer='uniform'))
model_T.add(Dense(units=nl*(nl+1), activation='relu', input_dim=npixok, kernel_initializer='uniform'))
model_T.add(Dense(units=nl, activation='linear'))

model_T.compile(optimizer='adam',
              loss='mse',
              metrics=['accuracy'])

# Training
fraction = 0.8
ilim = int(nbmodels*fraction)
print(ilim)

mxT = np.max(np.abs(mymaps))
myT = np.max(mycls)

from __future__ import print_function
class PrintNum(keras.callbacks.Callback):
  def on_epoch_end(self,epoch,logs):
    if epoch % 10 == 0: 
      print('')
      print(epoch, end='')
    sys.stdout.write('.')
    sys.stdout.flush()

early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)


historyT=model_T.fit(mymaps[0:ilim,:]/mxT, mycls[0:ilim]/myT, epochs=200, batch_size=1000, 
  validation_split=0.1, verbose=1, callbacks=[early_stop, PrintNum()])

# summarize history for loss
plot(historyT.history['loss'])
plot(historyT.history['val_loss'])
title('model loss')
ylabel('loss')
xlabel('epoch')
legend(['train', 'test'], loc='upper left')
yscale('log')
show()
print(min(historyT.history['loss']), min(historyT.history['val_loss']), len(historyT.history['val_loss']))


mymaps_test = mymaps[ilim:,:]
mycls_test = mycls[ilim:,:]
expcls_test = expcls[ilim:,:]

resultT = myT * model_T.predict(mymaps_test / mxT, batch_size=128)

clf()
num=np.random.randint(resultT.shape[0])
plot(lt, lt*(lt+1)*mycls_test[num,:],label ='Input spectra')
plot(lt, lt*(lt+1)*expcls_test[num,:],label ='Anafast')
plot(lt, lt*(lt+1)*resultT[num,:],label ='ML T')
plot(lt, lt*(lt+1)*result[num,:],label ='ML alm')
title(num)
legend()

clf()
a=hist(np.ravel(expcls_test[:,2:]-mycls_test[:,2:]), bins=100, range=[-5,5], alpha=0.5, label='Anafast')
a=hist(np.ravel(result[:,2:]-mycls_test[:,2:]), bins=100, range=[-5,5], alpha=0.5, label = 'ML alm')
a=hist(np.ravel(resultT[:,2:]-mycls_test[:,2:]), bins=100, range=[-5,5], alpha=0.5, label = 'ML T')
yscale('log')
legend()

ch2anafast = np.sum((expcls_test[:,2:]-mycls_test[:,2:])**2, axis=1)
ch2ML_T = np.sum((resultT[:,2:]-mycls_test[:,2:])**2, axis=1)

figure()
a=hist(ch2anafast, bins=100, range=[0,10000], alpha=0.5, label='Anafast')
a=hist(ch2ML, bins=100, range=[0,10000], alpha=0.5, label = 'ML alm')
a=hist(ch2ML_T, bins=100, range=[0,10000], alpha=0.5, label = 'ML T')
yscale('log')
legend()



